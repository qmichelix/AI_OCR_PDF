{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 spacy scikit-learn pandas\n",
    "\n",
    "!pip install PyPDF2\n",
    "!pip install pdfminer.six\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install openpyxl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    # Open the PDF file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        # Initialize a string to hold the text\n",
    "        text = ''\n",
    "        # Loop over the pages in the PDF file\n",
    "        for page in reader.pages:\n",
    "            # Extract the text from the page\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Test the function with a PDF file\n",
    "text = extract_text_from_pdf('./resume/CV_Michelix_Quentin_.pdf')\n",
    "print(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read pdf from a folder and transform them to text and save them into a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory you want to use\n",
    "dir_path = 'C:/Users/quentin.michelix/Desktop/ynov/data_resume/big_project/resume'\n",
    "\n",
    "# Get a list of all the PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(dir_path) if f.endswith('.pdf')]\n",
    "pdf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the text files\n",
    "output_dir = './text_resume/'\n",
    "\n",
    "# Loop over the PDF files\n",
    "for file in pdf_files:\n",
    "    # Extract the text from the PDF file\n",
    "    text = extract_text_from_pdf(os.path.join(dir_path, file))\n",
    "    \n",
    "    # Create a new text file and write the extracted text to it\n",
    "    with open(os.path.join(output_dir, file.replace('.pdf', '.txt')), 'w') as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les tokens et les balises a la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(columns=['file', 'text', 'company', 'region', 'stack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Specify the directory where your text files are\n",
    "dir_path = './text_resume/'\n",
    "\n",
    "# Get a list of all the text files in the directory\n",
    "text_files = [f for f in os.listdir(dir_path) if f.endswith('.txt')]\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(columns=['file', 'text', 'company', 'region', 'stack'])\n",
    "\n",
    "# Loop over the text files\n",
    "for file in text_files:\n",
    "    # Open the text file and read its contents\n",
    "    with open(os.path.join(dir_path, file), 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extract the entities\n",
    "    companies = re.findall(r'<company>(.*?)</company>', text)\n",
    "    regions = re.findall(r'<region>(.*?)</region>', text)\n",
    "    stacks = re.findall(r'<stack>(.*?)</stack>', text)\n",
    "\n",
    "    # Add the data to the DataFrame\n",
    "    df = df.append({'file': file, 'text': text, 'company': companies, 'region': regions, 'stack': stacks}, ignore_index=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('resume_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the text files\n",
    "for file in text_files:\n",
    "    # Open the text file and read its contents\n",
    "    with open(os.path.join(dir_path, file), 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extract the entities\n",
    "    companies = re.findall(r'<company>(.*?)</company>', text)\n",
    "    regions = re.findall(r'<region>(.*?)</region>', text)\n",
    "    stacks = re.findall(r'<stack>(.*?)</stack>', text)\n",
    "\n",
    "    # Add the data to the DataFrame\n",
    "    df = df.append({'file': file, 'text': text, 'company': companies, 'region': regions, 'stack': stacks}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/resume_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algo :\n",
    "\n",
    "NER is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're correct, you have three columns: 'company', 'region', and 'stack'. This means you're dealing with a multi-label classification problem, where each resume can have multiple labels.\n",
    "\n",
    "One approach to solve this is to train a separate classifier for each label. For example, you could train one classifier to predict the 'company', another to predict the 'region', and another to predict the 'stack'. Here's how you could modify the previous example to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df[['company', 'region', 'stack']], test_size=0.2)\n",
    "\n",
    "# Create a pipeline for each label\n",
    "company_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "region_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "stack_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Train each pipeline on the training data\n",
    "company_pipeline.fit(X_train, y_train['company'])\n",
    "region_pipeline.fit(X_train, y_train['region'])\n",
    "stack_pipeline.fit(X_train, y_train['stack'])\n",
    "\n",
    "# Use the trained pipelines to make predictions on the validation data\n",
    "company_predictions = company_pipeline.predict(X_val)\n",
    "region_predictions = region_pipeline.predict(X_val)\n",
    "stack_predictions = stack_pipeline.predict(X_val)\n",
    "\n",
    "# Print the predictions\n",
    "print('Company predictions:', company_predictions)\n",
    "print('Region predictions:', region_predictions)\n",
    "print('Stack predictions:', stack_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume that `y_val` is your validation labels and `y_pred` are the predicted labels\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es, Named Entity Recognition (NER) can be used in your use case, but it requires a different kind of data preparation and modeling approach.\n",
    "\n",
    "In NER, the goal is to identify and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. In your case, you would be interested in categories like 'company', 'region', 'stack', etc.\n",
    "\n",
    "The first step in using NER is to prepare your training data. You would need to annotate your resumes with the exact locations of the entities you're interested in. There are several tools available for this, such as Doccano, an open source text annotation tool for machine learning.\n",
    "\n",
    "Once you have your annotated data, you can train a NER model. There are several libraries that can help with this, such as spaCy or the Hugging Face's Transformers library. These libraries provide pre-trained models that you can fine-tune on your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the directory where your text files are\n",
    "dir_path = './text_resume/'\n",
    "\n",
    "# Get a list of all the text files in the directory\n",
    "text_files = [f for f in os.listdir(dir_path) if f.endswith('.txt')]\n",
    "\n",
    "# Prepare a list to store the training data\n",
    "train_data = []\n",
    "\n",
    "# Define a function to convert your annotations into the format required by spaCy\n",
    "def convert_annotations(text, label):\n",
    "    pattern = f'<{label}>(.*?)</{label}>'\n",
    "    matches = re.finditer(pattern, text)\n",
    "    entities = [(match.start(), match.end(), label) for match in matches]\n",
    "    return entities\n",
    "\n",
    "# Loop over the text files\n",
    "for file in text_files:\n",
    "    # Open the text file and read its contents\n",
    "    with open(os.path.join(dir_path, file), 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Convert the annotations\n",
    "    entities = convert_annotations(text, 'company')\n",
    "    entities += convert_annotations(text, 'region')\n",
    "    entities += convert_annotations(text, 'stack')\n",
    "\n",
    "    # Add the text and the annotations to the training data\n",
    "    train_data.append((text, {'entities': entities}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import random\n",
    "\n",
    "# Load a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add the NER component to the pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add your labels to the NER component\n",
    "for label in ['company', 'region', 'stack']:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Get the names of the other pipes in the pipeline, to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "# Start training\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    # Reset and initialize the weights randomly\n",
    "    nlp.begin_training()\n",
    "    for itn in range(100):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for batch in minibatch(train_data, size=compounding(4.0, 32.0, 1.001)):\n",
    "            examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch]\n",
    "            nlp.update(examples, drop=0.5, losses=losses)\n",
    "        print(\"Losses\", losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Scikit-Learn's metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use the model to predict the entities in the validation set\n",
    "y_pred = [nlp(text).ents for text in X_val]\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict entities in a new text\n",
    "doc = nlp(\"Some new text...\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have trained your model, the next steps would be to evaluate its performance and then use it for prediction. Here's how you can do it:\n",
    "\n",
    "Evaluate the model: You should have a separate dataset (or a portion of your dataset) that the model has not seen during training. This is your validation or test set. You can use this dataset to evaluate how well your model is performing. In the context of Named Entity Recognition, common evaluation metrics include precision, recall, and F1 score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do that, i have another repertory called \"new_resume\" where there are multiple new unknown resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Directory where the new resumes are stored\n",
    "new_resumes_dir = \"./new_resume\"\n",
    "\n",
    "# List to store the texts of the new resumes\n",
    "new_resumes_text = []\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(new_resumes_dir):\n",
    "    # Check if the file is a PDF\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Open the PDF file\n",
    "        with open(os.path.join(new_resumes_dir, filename), \"rb\") as file:\n",
    "            # Use PyPDF2 to extract the text\n",
    "            pdf = PdfReader(file)\n",
    "            text = \" \".join(page.extract_text() for page in pdf.pages)\n",
    "            new_resumes_text.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the texts of the new resumes\n",
    "test_df = pd.DataFrame(new_resumes_text, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the predicted entities\n",
    "predicted_entities = []\n",
    "\n",
    "# Iterate over the texts in the test dataset\n",
    "for text in test_df[\"text\"]:\n",
    "    # Use the model to make predictions\n",
    "    doc = nlp(text)\n",
    "    # Extract the entities from the doc\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    predicted_entities.append(entities)\n",
    "\n",
    "# Add the predicted entities to the test DataFrame\n",
    "test_df[\"predicted_entities\"] = predicted_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the test DataFrame\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of entities to a string so it can be written to Excel\n",
    "test_df[\"predicted_entities\"] = test_df[\"predicted_entities\"].apply(str)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "test_df.to_excel(\"predicted_entities.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ynov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
